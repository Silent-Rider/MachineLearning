import random
from keras import models, layers
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np
from helper_utils import vectorize_sequences


def readFile(filepath, text_labels:dict):
    sentences: list[str] = []
    labels: list[int] = []
    current_genre = None

    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue

            if line.startswith("[") and line.endswith("]"):
                genre_name = line[1:-1]
                if genre_name in text_labels:
                    current_genre = genre_name
                else:
                    raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∂–∞–Ω—Ä: {genre_name}")
            else:
                if current_genre is None:
                    raise ValueError("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∂–∞–Ω—Ä–∞!")
                line = line.replace(" –æ–Ω ", " ")
                line = line.replace(" —è ", " ")
                line = line.replace(" –æ–Ω–∞ ", " ")
                line = line.replace(" –æ–Ω–∏ ", " ")
                line = line.replace(" –µ–≥–æ ", " ")
                line = line.replace(" –º–µ–Ω—è ", " ")
                line = line.replace(" –µ—ë ", " ")
                line = line.replace(" –µ–µ ", " ")
                line = line.replace(" –∏—Ö ", " ")
                line = line.replace(" –µ–º—É ", " ")
                line = line.replace(" –º–Ω–µ ", " ")
                line = line.replace(" –µ–π ", " ")
                line = line.replace(" –∏–º ", " ")
                line = line.replace(" –æ –Ω–µ–º ", " ")
                line = line.replace(" –æ–±–æ –º–Ω–µ ", " ")
                line = line.replace(" –æ –Ω—ë–º ", " ")
                line = line.replace(" –æ –Ω–µ–π ", " ")

                sentences.append(line)
                labels.append(text_labels[current_genre])
    return sentences, labels


def generate_data(filepath) -> tuple[list[str], list[int]]:
    genre_to_label:dict = {
        "—É–∂–∞—Å—ã": 0,
        "–∫–æ–º–µ–¥–∏—è": 1,
        "—Ñ–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞": 2,
        "–¥—Ä–∞–º–∞": 3,
        "–±–æ–µ–≤–∏–∫": 4
    }
    sentences, labels = readFile(filepath, genre_to_label)

    combined = list(zip(sentences, labels))
    random.seed(42)
    random.shuffle(combined)
    sentences, labels = zip(*combined)

    return sentences, labels


tokenizer = Tokenizer()
(raw_data, raw_labels) = generate_data("movie_genres.txt")
tokenizer.fit_on_texts(raw_data)
train_data = vectorize_sequences(tokenizer.texts_to_sequences(raw_data), 1000)
train_labels = np.asarray(raw_labels).astype('float32')

x_val = train_data[:10]
x_train = train_data[10:]
y_val = train_labels[:10]
y_train = train_labels[10:]

model = models.Sequential()
model.add(layers.Dense(32, activation='relu', input_shape=(1000,)))
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(5, activation='softmax'))

model.compile(optimizer='rmsprop',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(
    x_train,
    y_train,
    epochs=50,
    batch_size=16,
    validation_data=(x_val, y_val)
)

while True:
    review = input("–í–≤–µ–¥–∏—Ç–µ –æ—Ç–∑—ã–≤ –∫ —Ñ–∏–ª—å–º—É (–∏–ª–∏ '–≤—ã—Ö–æ–¥' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è): ")
    if review.lower() == '–≤—ã—Ö–æ–¥':
        break
    seq = tokenizer.texts_to_sequences([review])
    vec = vectorize_sequences(seq, dimension=1000)

    prediction = model.predict(vec)
    genre_id = np.argmax(prediction[0])

    genre:str = ''
    if 0 <= genre_id < 5:
        match genre_id:
            case 0: genre = 'üò± –£–∂–∞—Å—ã'
            case 1: genre = 'üòÇ –ö–æ–º–µ–¥–∏—è'
            case 2: genre = 'üöÄ –§–∞–Ω—Ç–∞—Å—Ç–∏–∫–∞'
            case 3: genre = 'üíî –î—Ä–∞–º–∞'
            case 4: genre = 'üí• –ë–æ–µ–≤–∏–∫'

    print(f"–ñ–∞–Ω—Ä: {genre} –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ: {np.max(prediction[0]):.4f}\n")