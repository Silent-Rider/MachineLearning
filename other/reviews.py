from keras import models, layers
from tensorflow.keras.preprocessing.text import Tokenizer
import numpy as np
from helper_utils import vectorize_sequences

def generate_data(token) -> tuple[np.ndarray, np.ndarray]:
    texts = ["ะะต ะปัะฑะปั ัะฐะบะพะต",
             "ะะฝะต ะฝัะฐะฒะธััั ัะธะปัะผ",
             "ะะฝะต ะฝะต ะฝัะฐะฒะธััั ัะธะปัะผ",
             "ะฏ ะฝะตะฝะฐะฒะธะถั ััะพั ัะธะปัะผ",
             "ะฏ ะพะฑะพะถะฐั ััะพั ัะธะปัะผ",
             "ะคะธะปัะผ ะปัััะธะน",
             "ะะฝะต ะพัะตะฝั ะฝัะฐะฒะธััั ััะพั ัะธะปัะผ",
             "ะฏ ะปัะฑะปั ััะพั ัะธะปัะผ",
             "ะคะธะปัะผ ัะถะฐัะตะฝ",
             "ะฅะพัะพัะพ",
             "ะคะธะปัะผ ะฟะปะพั",
             "ะคะธะปัะผ ัะธะบะฐัะฝัะน",
             "ะฃะถะฐัะฝะพ ะฟะปะพัะพะน ัะธะปัะผ",
             "ะะฝัะตัะตัะฝัะน ัะธะปัะผ",
             "ะะฝัะตัะตัะฝัะน",
             "ะะฝัะตัะตัะฝัะน ััะตะฝะฐัะธะน",
             "ะัะฐะฒะธััั ะฟะพะดะพะฑะฝัะน ะถะฐะฝั",
             "ะะตะฝะฐะฒะธะถั ะฟะพะดะพะฑะฝะพะต",
             "ะคะธะปัะผ ัะพัะพั"]

    token.fit_on_texts(texts)
    data = vectorize_sequences(token.texts_to_sequences(texts), 100)
    labels = np.asarray([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1]).astype('float32')
    return data, labels

tokenizer = Tokenizer()
train_data, train_labels = generate_data(tokenizer)

model = models.Sequential()
model.add(layers.Dense(16, activation='relu', input_shape=(100,)))
model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

x_val = train_data[:1]           # 1 ะฟัะธะผะตั ะฝะฐ ะฒะฐะปะธะดะฐัะธั
partial_x_train = train_data[1:] # 18 ะฟัะธะผะตัะฐ ะฝะฐ ะพะฑััะตะฝะธะต
y_val = train_labels[:1]
partial_y_train = train_labels[1:]

history = model.fit(partial_x_train,
    partial_y_train,
    epochs=200,
    batch_size=3,
    validation_data=(x_val, y_val))

while True:
    review = input("ะะฒะตะดะธัะต ะพัะทัะฒ (ะธะปะธ 'ะฒััะพะด' ะดะปั ะทะฐะฒะตััะตะฝะธั): ")
    if review.lower() == 'ะฒััะพะด':
        break
    seq = tokenizer.texts_to_sequences([review])
    vec = vectorize_sequences(seq, dimension=100)
    pred = model.predict(vec)[0][0]
    print(f"ะัะตะฝะบะฐ: {pred:.4f} โ {'๐ ะะพะทะธัะธะฒะฝัะน' if pred > 0.5 else '๐ ะะตะณะฐัะธะฒะฝัะน'}\n")





